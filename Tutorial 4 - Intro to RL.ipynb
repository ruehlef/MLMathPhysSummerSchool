{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "becd47a4-40fd-4a11-ba4e-cdd7fd054a7d",
   "metadata": {},
   "source": [
    "# Tutorial 4 - Intro to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06829dc0-13fb-4555-8a66-a357cb747444",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn the basics of RL. We use stable baselines 3 for the RL implementation and just implement the environment.\n",
    "\n",
    "We will use a 1D spin chain with $N$ lattice sites as an example:\n",
    "* States: The spins at each lattice site can point either up or down\n",
    "* Actions: Flip the orientation of a spin at site $n$, $1\\leq n \\leq N$\n",
    "* Terminal state: The minimal energy configuration, which is obtained when all spins are aligned\n",
    "* Reward: punish by the energy of the configuration\n",
    "\n",
    "The energy is \n",
    "$$ E = J \\sum_i s_i s_{i+1}$$\n",
    "where $J\\in\\mathbb{R}$ measures the coupling strength and $s_i\\in[-1, 1]$ encodes spin up/down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "548e6ae5-05c1-4069-836d-c8f35d1527c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4302c6b-b139-4e9a-9bd5-7fb63513f103",
   "metadata": {},
   "source": [
    "The gym environment needs to implement 4 methods:\n",
    "* step(): Updates an environment with actions returning the next agent observation, the reward for taking that actions, if the environment has terminated or truncated due to the latest action and information from the environment about the step, i.e. metrics, debug info.\n",
    "* reset() - Resets the environment to an initial state, required before calling step. Returns the first agent observation for an episode and information, i.e. metrics, debug info.\n",
    "* render() - Renders the environments to help visualise what the agent see, examples modes are “human”, “rgb_array”, “ansi” for text.\n",
    "* close() - Closes the environment, important when external software is used, i.e. pygame for rendering, databases\n",
    "\n",
    "Also, the class should have two members:\n",
    "* action_space\n",
    "* observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd81a639-e1d1-41cc-82cb-5e72b3ffc3e2",
   "metadata": {},
   "source": [
    "## 1.) Define the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a2330e-ab6e-46a2-afa4-69653e895e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpinChain(gym.Env):\n",
    "    render_modes = [\"ansi\"]\n",
    "    metadata = {\"render_modes\": render_modes, \"render_fps\": 1}\n",
    "    \n",
    "    def __init__(self, N, J=1, render_mode=\"ansi\"):\n",
    "        super(SpinChain, self).__init__()\n",
    "        self.actions = []\n",
    "        self.J = J\n",
    "        self.N = N\n",
    "        self.state = np.random.randint(low=0, high=2, size=self.N)\n",
    "        self.action_space = gym.spaces.Discrete(self.N)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(self.N,), dtype=np.float32)\n",
    "        self.render_mode = render_mode\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.actions.append(action)\n",
    "        # carry out action: Flip spin at site \"action\"\n",
    "        self.state[action] = (self.state[action] + 1) % 2\n",
    "        reward, terminated, truncated, info = self.reward(), False, False, {}\n",
    "\n",
    "        # We have two termnianl states: The one where all spins are pointing in the same direction\n",
    "        if all(self.state.astype(bool)) or not any(self.state.astype(bool)):\n",
    "            terminated = True\n",
    "            info = {\"message\": f\"Found the minimum energy configuration with actions {self.actions}.\"}\n",
    "        \n",
    "        # We truncate the game after 300 steps\n",
    "        if len(self.actions) == 300:\n",
    "            truncated = True\n",
    "            info = {\"message\": f\"Ended the episode after 300 steps.\"}\n",
    "        \n",
    "        return self.prepare_state(), reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.state = np.random.randint(low=0, high=2, size=self.N)\n",
    "        self.actions = []\n",
    "        info = {\"message\": f\"Reset to start state {self.render()} with energy {self.reward()}\"}\n",
    "        return self.prepare_state(), info\n",
    "\n",
    "    def render(self):\n",
    "        state_dict = {0: \"↑\", 1: \"↓\"}\n",
    "        return \" \".join([state_dict[x] for x in self.state])\n",
    "        \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def reward(self):\n",
    "        # to compute the energy we want to map spin up to -1 and spin down to +1\n",
    "        spins = np.array([(-1)**x for x in self. state], dtype=np.float32)\n",
    "        energy = self.J * np.sum(spins[:-1] * spins[1:])  # nearest-neighbor interaction\n",
    "        return energy\n",
    "\n",
    "    def prepare_state(self):\n",
    "        return self.state.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e9eafb-bea7-4bc2-bd6d-744111b302a4",
   "metadata": {},
   "source": [
    "## 2.) Investigate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e4779c-5b2e-4a04-b684-39adbf4dfc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 0. 0. 0. 1. 1.] {'message': 'Reset to start state ↓ ↓ ↓ ↓ ↓ ↑ ↑ ↑ ↓ ↓ with energy 5.0'}\n",
      "↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑: 9.0\n",
      "↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 9.0\n"
     ]
    }
   ],
   "source": [
    "my_env = SpinChain(10)\n",
    "state, info = my_env.reset()\n",
    "print(state, info)\n",
    "\n",
    "# energy of all spins up:\n",
    "my_env.state = np.array([0] * my_env.N)\n",
    "print(f\"{my_env.render()}: {my_env.reward()}\")\n",
    "# energy of all spins down:\n",
    "my_env.state = np.array([1] * my_env.N)\n",
    "print(f\"{my_env.render()}: {my_env.reward()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29ca7417-5996-44d3-b750-a5c3824b8d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 [0. 1. 1. 1. 1. 1. 1. 0. 0. 1.] 3.0 False False {}\n",
      "7 [0. 1. 1. 1. 1. 1. 1. 1. 0. 1.] 3.0 False False {}\n",
      "6 [0. 1. 1. 1. 1. 1. 0. 1. 0. 1.] -1.0 False False {}\n",
      "0 [1. 1. 1. 1. 1. 1. 0. 1. 0. 1.] 1.0 False False {}\n",
      "7 [1. 1. 1. 1. 1. 1. 0. 0. 0. 1.] 5.0 False False {}\n",
      "7 [1. 1. 1. 1. 1. 1. 0. 1. 0. 1.] 1.0 False False {}\n",
      "1 [1. 0. 1. 1. 1. 1. 0. 1. 0. 1.] -3.0 False False {}\n",
      "0 [0. 0. 1. 1. 1. 1. 0. 1. 0. 1.] -1.0 False False {}\n",
      "9 [0. 0. 1. 1. 1. 1. 0. 1. 0. 0.] 1.0 False False {}\n",
      "0 [1. 0. 1. 1. 1. 1. 0. 1. 0. 0.] -1.0 False False {}\n"
     ]
    }
   ],
   "source": [
    "# walk around in state space by performing a few random actions\n",
    "my_env.reset()\n",
    "for _ in range(10):\n",
    "    action = np.random.randint(my_env.N)\n",
    "    state, reward, terminated, truncated, info = my_env.step(action)\n",
    "    print(action, state, reward, terminated, truncated, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b9dfe-4166-45b3-9d2e-46b107066a0a",
   "metadata": {},
   "source": [
    "## 3.) Connect to stable baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dfdb138-135c-4aec-8b38-30e536884f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04fb05c182a4334ab46bee7447628b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 249      |\n",
      "|    ep_rew_mean            | 20.8     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 2215     |\n",
      "|    iterations             | 4        |\n",
      "|    time_elapsed           | 3        |\n",
      "|    total_timesteps        | 8192     |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.0016   |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00409  |\n",
      "|    learning_rate          | 0.0001   |\n",
      "|    n_updates              | 3        |\n",
      "|    policy_objective       | 0.0178   |\n",
      "|    value_loss             | 259      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 244       |\n",
      "|    ep_rew_mean            | 7.82      |\n",
      "| time/                     |           |\n",
      "|    fps                    | 2221      |\n",
      "|    iterations             | 8         |\n",
      "|    time_elapsed           | 7         |\n",
      "|    total_timesteps        | 16384     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -0.000979 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00455   |\n",
      "|    learning_rate          | 0.0001    |\n",
      "|    n_updates              | 7         |\n",
      "|    policy_objective       | 0.0207    |\n",
      "|    value_loss             | 347       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 241      |\n",
      "|    ep_rew_mean            | 18.6     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 2214     |\n",
      "|    iterations             | 12       |\n",
      "|    time_elapsed           | 11       |\n",
      "|    total_timesteps        | 24576    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.0106   |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.0053   |\n",
      "|    learning_rate          | 0.0001   |\n",
      "|    n_updates              | 11       |\n",
      "|    policy_objective       | 0.0229   |\n",
      "|    value_loss             | 317      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 244      |\n",
      "|    ep_rew_mean            | 87.2     |\n",
      "| time/                     |          |\n",
      "|    fps                    | 2202     |\n",
      "|    iterations             | 16       |\n",
      "|    time_elapsed           | 14       |\n",
      "|    total_timesteps        | 32768    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.00457  |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00511  |\n",
      "|    learning_rate          | 0.0001   |\n",
      "|    n_updates              | 15       |\n",
      "|    policy_objective       | 0.0295   |\n",
      "|    value_loss             | 530      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 254      |\n",
      "|    ep_rew_mean            | 282      |\n",
      "| time/                     |          |\n",
      "|    fps                    | 2199     |\n",
      "|    iterations             | 20       |\n",
      "|    time_elapsed           | 18       |\n",
      "|    total_timesteps        | 40960    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.000482 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00526  |\n",
      "|    learning_rate          | 0.0001   |\n",
      "|    n_updates              | 19       |\n",
      "|    policy_objective       | 0.0333   |\n",
      "|    value_loss             | 1.85e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 268      |\n",
      "|    ep_rew_mean            | 573      |\n",
      "| time/                     |          |\n",
      "|    fps                    | 2199     |\n",
      "|    iterations             | 24       |\n",
      "|    time_elapsed           | 22       |\n",
      "|    total_timesteps        | 49152    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 6.14e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00545  |\n",
      "|    learning_rate          | 0.0001   |\n",
      "|    n_updates              | 23       |\n",
      "|    policy_objective       | 0.031    |\n",
      "|    value_loss             | 4.27e+03 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 285      |\n",
      "|    ep_rew_mean            | 923      |\n",
      "| time/                     |          |\n",
      "|    fps                    | 2195     |\n",
      "|    iterations             | 28       |\n",
      "|    time_elapsed           | 26       |\n",
      "|    total_timesteps        | 57344    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 4.04e-05 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00565  |\n",
      "|    learning_rate          | 0.0001   |\n",
      "|    n_updates              | 27       |\n",
      "|    policy_objective       | 0.02     |\n",
      "|    value_loss             | 6.86e+03 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 295       |\n",
      "|    ep_rew_mean            | 1.27e+03  |\n",
      "| time/                     |           |\n",
      "|    fps                    | 2188      |\n",
      "|    iterations             | 32        |\n",
      "|    time_elapsed           | 29        |\n",
      "|    total_timesteps        | 65536     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -8.34e-07 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00535   |\n",
      "|    learning_rate          | 0.0001    |\n",
      "|    n_updates              | 31        |\n",
      "|    policy_objective       | 0.018     |\n",
      "|    value_loss             | 8.17e+03  |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 300      |\n",
      "|    ep_rew_mean            | 1.53e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 2182     |\n",
      "|    iterations             | 36       |\n",
      "|    time_elapsed           | 33       |\n",
      "|    total_timesteps        | 73728    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 2.98e-07 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00681  |\n",
      "|    learning_rate          | 0.0001   |\n",
      "|    n_updates              | 35       |\n",
      "|    policy_objective       | 0.0152   |\n",
      "|    value_loss             | 9.86e+03 |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                  |           |\n",
      "|    ep_len_mean            | 297       |\n",
      "|    ep_rew_mean            | 1.69e+03  |\n",
      "| time/                     |           |\n",
      "|    fps                    | 2176      |\n",
      "|    iterations             | 40        |\n",
      "|    time_elapsed           | 37        |\n",
      "|    total_timesteps        | 81920     |\n",
      "| train/                    |           |\n",
      "|    explained_variance     | -2.86e-06 |\n",
      "|    is_line_search_success | 1         |\n",
      "|    kl_divergence_loss     | 0.00458   |\n",
      "|    learning_rate          | 0.0001    |\n",
      "|    n_updates              | 39        |\n",
      "|    policy_objective       | 0.0124    |\n",
      "|    value_loss             | 1.1e+04   |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 294      |\n",
      "|    ep_rew_mean            | 1.79e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 2172     |\n",
      "|    iterations             | 44       |\n",
      "|    time_elapsed           | 41       |\n",
      "|    total_timesteps        | 90112    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 2.8e-06  |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.00385  |\n",
      "|    learning_rate          | 0.0001   |\n",
      "|    n_updates              | 43       |\n",
      "|    policy_objective       | 0.0144   |\n",
      "|    value_loss             | 1.17e+04 |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                  |          |\n",
      "|    ep_len_mean            | 294      |\n",
      "|    ep_rew_mean            | 1.88e+03 |\n",
      "| time/                     |          |\n",
      "|    fps                    | 2169     |\n",
      "|    iterations             | 48       |\n",
      "|    time_elapsed           | 45       |\n",
      "|    total_timesteps        | 98304    |\n",
      "| train/                    |          |\n",
      "|    explained_variance     | 0.000204 |\n",
      "|    is_line_search_success | 1        |\n",
      "|    kl_divergence_loss     | 0.004    |\n",
      "|    learning_rate          | 0.0001   |\n",
      "|    n_updates              | 47       |\n",
      "|    policy_objective       | 0.00779  |\n",
      "|    value_loss             | 1.23e+04 |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training.\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.envs.registration import register\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from sb3_contrib import TRPO\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10000,                # save every 10,000 steps\n",
    "    save_path=\"./checkpoints/\",    # folder to save models\n",
    "    name_prefix=\"trpo_model\",      # filename prefix\n",
    "    save_replay_buffer=False,      \n",
    "    save_vecnormalize=False        \n",
    ")\n",
    "\n",
    "register(\n",
    "    id='SpinChain-v0',\n",
    "    entry_point='__main__:SpinChain',  # 'module_path:ClassName'\n",
    ")\n",
    "\n",
    "env = gym.make('SpinChain-v0', N=10, render_mode=\"ansi\")\n",
    "\n",
    "model = TRPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    device='cpu',  # CUDA for TRPO is only recommended for CNN policies\n",
    "    env=env,\n",
    "    gamma=.995,\n",
    "    learning_rate=1e-4,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"Start training.\")\n",
    "model.learn(total_timesteps=100000, log_interval=4, progress_bar=True, callback=checkpoint_callback)\n",
    "print(\"Done training.\")\n",
    "model.save(\"trpo_spinchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1b307e2-48c2-44ce-a630-797ac173ed98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↑ ↓ ↑: 1.0\n",
      " 9 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↑ ↓ ↓: 3.0\n",
      " 7 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n",
      " 3 ↑ ↑ ↑ ↓ ↓ ↓ ↓ ↓ ↓ ↓: 7.0\n"
     ]
    }
   ],
   "source": [
    "model = TRPO.load(\"trpo_spinchain\", device='cpu')\n",
    "\n",
    "obs, _ = env.reset()\n",
    "for _ in range(20):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"{action:2d} {env.render()}: {env.unwrapped.reward()}\")\n",
    "    if terminated or truncated:\n",
    "        print(info)\n",
    "        obs, _ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755b7b9-c627-4540-a7d9-3241cd76a49c",
   "metadata": {},
   "source": [
    "# Now it's your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1843f48-cd22-4cf0-9473-2d1e43358fe8",
   "metadata": {},
   "source": [
    "I list some suggestions of what you could do below. Pick the one (or ones) that intrest you the most, or just play with the notebook and investigate your own questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251adb8-3fec-4939-8d21-6d43f93aebf1",
   "metadata": {},
   "source": [
    "## Exercise 1: Play with the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de04cdb-3550-4afc-8ec9-eaa3c80798cc",
   "metadata": {},
   "source": [
    "* Play with spin chains of different lengths\n",
    "* Implement curriculum learning, where instead of getting a random spin chain the reset function generates spin chains with 1 spin flipped, then with 2, etc.\n",
    "* Change the action space such that we have 2N actions, where the first N actions set the nth spin to up and the next N actions set the nth spin to down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7c393-b60c-494e-84bc-33dbc2671584",
   "metadata": {},
   "source": [
    "## Exercise 2: Try other algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6472e-7356-4047-98e6-c506e9e0da39",
   "metadata": {},
   "source": [
    "Try other RL algorithms as implemented in stable baselines 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69704e65-ccdd-4509-9a91-d2d06bd28022",
   "metadata": {},
   "source": [
    "## Exercise 3: 2D spin chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d438ef-438c-4470-a0fe-e52063f0cedf",
   "metadata": {},
   "source": [
    "Modify the environment to a 2D lattice and use a CNN for the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d369d9c2-cd65-4475-87a9-0654e5a2d55f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-rl",
   "language": "python",
   "name": "venv-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
